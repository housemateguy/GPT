{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT Language Model Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSO7x7TmmpN4",
        "outputId": "27ee0daf-5dde-442a-a933-21637c1ac04d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov  2 20:33:55 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT-Ac6DdmeZa",
        "outputId": "07a7ec3b-76cc-4395-c0a0-e56aa52ea67d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -qq wandb\n",
        "!git clone https://github.com/lattice-ai/GPT\n",
        "%cd GPT"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/GPT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU8AkAKXmvPd"
      },
      "source": [
        "from gpt.experiments.language_model import IMDBReviewLanguageExperiment\n",
        "\n",
        "experiment = IMDBReviewLanguageExperiment()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OlyPwD3iLJb",
        "outputId": "cbdb2068-f98c-4c29-e427-2830e3bcee4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from gpt.experiments.utils import init_wandb\n",
        "\n",
        "init_wandb(\n",
        "    project_name='gpt', experiment_name='imdb_language_model',\n",
        "    wandb_api_key='41442f74323655b7ec081f4af030fa821f294c72'\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlattice-ai\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.8<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">imdb_language_model</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/lattice-ai/gpt\" target=\"_blank\">https://wandb.ai/lattice-ai/gpt</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/lattice-ai/gpt/runs/3lk64c01\" target=\"_blank\">https://wandb.ai/lattice-ai/gpt/runs/3lk64c01</a><br/>\n",
              "                Run data is saved locally in <code>wandb/run-20201102_203410-3lk64c01</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8htcxsbOm1XO",
        "outputId": "342eb5c1-77b6-44c7-a047-020dd308d4b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "experiment.build_dataset(\n",
        "    'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n",
        "\n",
        "experiment.compile()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Size: 50000 files\n",
            "Dataset: <PrefetchDataset shapes: ((None, 100), (None, 100)), types: (tf.int64, tf.int64)>\n",
            "Model: \"GPT_depth_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "GPT_Input (InputLayer)       [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "combined_encoding (CombinedE (None, 100, 256)          5145600   \n",
            "_________________________________________________________________\n",
            "transformer_block (Transform (None, 100, 256)          395776    \n",
            "_________________________________________________________________\n",
            "GPT_Output (Dense)           (None, 100, 20000)        5140000   \n",
            "=================================================================\n",
            "Total params: 10,681,376\n",
            "Trainable params: 10,681,376\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWo7l9uam4cq",
        "outputId": "9e9f5230-7697-4b08-cf04-99fe6f864ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start_text = 'the actor was'\n",
        "start_tokens = experiment.tokenize(start_text=start_text)\n",
        "experiment.train(\n",
        "    epochs=30, start_tokens=start_tokens,\n",
        "    max_length=100, max_tokens=40, top_k=10,\n",
        "    infer_every=1, log_on_wandb=True\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "   1563/Unknown - 144s 92ms/step - loss: 5.0512 - GPT_Output_loss: 5.0512\n",
            "Sample Generate Text: the actor was very believable . . . . . the film is very good , the plot is not even the movie itself . it 's very hard to get a bit of it . but the plot has a very bad taste very believable . . . . . the film is very good , the plot is not even the movie itself . it 's very hard to get a bit of it . but the plot has a very bad taste\n",
            "\n",
            "Epoch 00001: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint1\n",
            "1563/1563 [==============================] - 146s 93ms/step - loss: 5.0512 - GPT_Output_loss: 5.0512\n",
            "Epoch 2/30\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 4.4895 - GPT_Output_loss: 4.4895\n",
            "Sample Generate Text: the actor was a [UNK] , this is a very good film . the acting was terrible . . it was bad and bad . it was bad . . . . . .the film is good enough , the acting , the acting a [UNK] , this is a very good film . the acting was terrible . . it was bad and bad . it was bad . . . . . .the film is good enough , the acting , the acting\n",
            "\n",
            "Epoch 00002: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint2\n",
            "1563/1563 [==============================] - 145s 93ms/step - loss: 4.4895 - GPT_Output_loss: 4.4895\n",
            "Epoch 3/30\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 4.2955 - GPT_Output_loss: 4.2955\n",
            "Sample Generate Text: the actor was a great actor and his performance as [UNK] [UNK] . he was in a very good actor . he was a great actor for a big [UNK] and a [UNK] . he was a very funny person who had a great a great actor and his performance as [UNK] [UNK] . he was in a very good actor . he was a great actor for a big [UNK] and a [UNK] . he was a very funny person who had a great\n",
            "\n",
            "Epoch 00003: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint3\n",
            "1563/1563 [==============================] - 144s 92ms/step - loss: 4.2955 - GPT_Output_loss: 4.2955\n",
            "Epoch 4/30\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 4.1644 - GPT_Output_loss: 4.1644\n",
            "Sample Generate Text: the actor was great too ! i had never heard of this film was not a very good actor . the story was very good . i was a good director , but he wasn 't really that good . i liked him in great too ! i had never heard of this film was not a very good actor . the story was very good . i was a good director , but he wasn 't really that good . i liked him in\n",
            "\n",
            "Epoch 00004: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint4\n",
            "1563/1563 [==============================] - 144s 92ms/step - loss: 4.1644 - GPT_Output_loss: 4.1644\n",
            "Epoch 5/30\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 4.0627 - GPT_Output_loss: 4.0627\n",
            "Sample Generate Text: the actor was a little too bad for this film , not as good as a good actor . i don 't think this film is a complete disappointment . a lot of action and the plot is just plain terrible . the plot a little too bad for this film , not as good as a good actor . i don 't think this film is a complete disappointment . a lot of action and the plot is just plain terrible . the plot\n",
            "\n",
            "Epoch 00005: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint5\n",
            "1563/1563 [==============================] - 144s 92ms/step - loss: 4.0627 - GPT_Output_loss: 4.0627\n",
            "Epoch 6/30\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 3.9792 - GPT_Output_loss: 3.9792\n",
            "Sample Generate Text: the actor was excellent . the film was good , but i really like the other movies . it was really good , but this film is so good it is just a very touching story about the family that is played by emily excellent . the film was good , but i really like the other movies . it was really good , but this film is so good it is just a very touching story about the family that is played by emily\n",
            "\n",
            "Epoch 00006: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint6\n",
            "1563/1563 [==============================] - 144s 92ms/step - loss: 3.9792 - GPT_Output_loss: 3.9792\n",
            "Epoch 7/30\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 3.9089 - GPT_Output_loss: 3.9089\n",
            "Sample Generate Text: the actor was really bad . the director was one of the best of my most favorite actors i have ever seen . he did a great job . there are also a few things in this movie , which is the same movie really bad . the director was one of the best of my most favorite actors i have ever seen . he did a great job . there are also a few things in this movie , which is the same movie\n",
            "\n",
            "Epoch 00007: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint7\n",
            "1563/1563 [==============================] - 143s 92ms/step - loss: 3.9089 - GPT_Output_loss: 3.9089\n",
            "Epoch 8/30\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 3.8487 - GPT_Output_loss: 3.8487\n",
            "Sample Generate Text: the actor was great as it was [UNK] as the title , the title role , as the villain who wrote the script should have been done . the plot was a little better . the film was the plot , a bit dodgy great as it was [UNK] as the title , the title role , as the villain who wrote the script should have been done . the plot was a little better . the film was the plot , a bit dodgy\n",
            "\n",
            "Epoch 00008: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint8\n",
            "1563/1563 [==============================] - 143s 91ms/step - loss: 3.8487 - GPT_Output_loss: 3.8487\n",
            "Epoch 9/30\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 3.7955 - GPT_Output_loss: 3.7955\n",
            "Sample Generate Text: the actor was not a great actor . i think it 's one of the best movies in my opinion that this movie is a disgrace to the name and this one . the most important part of the cast are great . a not a great actor . i think it 's one of the best movies in my opinion that this movie is a disgrace to the name and this one . the most important part of the cast are great . a\n",
            "\n",
            "Epoch 00009: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint9\n",
            "1563/1563 [==============================] - 144s 92ms/step - loss: 3.7955 - GPT_Output_loss: 3.7955\n",
            "Epoch 10/30\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 3.7494 - GPT_Output_loss: 3.7494\n",
            "Sample Generate Text: the actor was the funniest [UNK] movie ! the first 10 seconds i 've seen in a year , and there were more than 2 minutes . the film had me hooked on the edge of my seat throughout the whole movie . a the funniest [UNK] movie ! the first 10 seconds i 've seen in a year , and there were more than 2 minutes . the film had me hooked on the edge of my seat throughout the whole movie . a\n",
            "\n",
            "Epoch 00010: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint10\n",
            "1563/1563 [==============================] - 146s 93ms/step - loss: 3.7494 - GPT_Output_loss: 3.7494\n",
            "Epoch 11/30\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 3.7075 - GPT_Output_loss: 3.7075\n",
            "Sample Generate Text: the actor was really bad , not as much in my opinion of this film is not a comedy . the only way that i think you have the only thing that makes this film more than a few of the movie is that really bad , not as much in my opinion of this film is not a comedy . the only way that i think you have the only thing that makes this film more than a few of the movie is that\n",
            "\n",
            "Epoch 00011: saving model to wandb/run-20201102_203410-3lk64c01/files/gpt_language_model_checkpoint11\n",
            "1563/1563 [==============================] - 145s 93ms/step - loss: 3.7075 - GPT_Output_loss: 3.7075\n",
            "Epoch 12/30\n",
            " 136/1563 [=>............................] - ETA: 2:13 - loss: 3.6847 - GPT_Output_loss: 3.6847"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-452b35afe95d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0minfer_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_on_wandb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m/content/GPT/gpt/experiments/language_model/large_movie_review/experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, start_tokens, max_length, max_tokens, top_k, infer_every, log_on_wandb)\u001b[0m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m         history = self.model.fit(\n\u001b[0;32m---> 76\u001b[0;31m             self.dataset, epochs=epochs, callbacks=callbacks)\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF8CUCxiD3ie",
        "outputId": "9311123e-25e1-4906-c59f-2d0038c6d543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start_text = 'the movie is'\n",
        "start_tokens = experiment.tokenize(start_text=start_text)\n",
        "output = experiment.infer(\n",
        "    start_tokens, max_length=100, max_tokens=40, top_k=10\n",
        ")\n",
        "print('Generated Text:', output)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated Text: the movie is excellent and the plot is interesting , and the acting is quite good and the plot is quite predictable . the plot is simple . it is not the acting . the only good character is that the script is so excellent and the plot is interesting , and the acting is quite good and the plot is quite predictable . the plot is simple . it is not the acting . the only good character is that the script is so\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mB4kYkuo1hm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}